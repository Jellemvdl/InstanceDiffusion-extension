"""
This code evaluates how well the global image captions generated by the LLM match the corresponding output images.
"""

import torch
import clip
from PIL import Image
import os
import json

def get_clip_score(image_path, text):
# Load the pre-trained CLIP model and the image
    model, preprocess = clip.load('ViT-B/32')
    image = Image.open(image_path)

    # Preprocess the image and tokenize the text
    image_input = preprocess(image).unsqueeze(0)
    text_input = clip.tokenize([text])
    
    # Move the inputs to GPU if available
    device = "cuda" if torch.cuda.is_available() else "cpu"
    image_input = image_input.to(device)
    text_input = text_input.to(device)
    model = model.to(device)
    
    # Generate embeddings for the image and text
    with torch.no_grad():
        image_features = model.encode_image(image_input)
        text_features = model.encode_text(text_input)
    
    # Normalize the features
    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
    text_features = text_features / text_features.norm(dim=-1, keepdim=True)
    
    # Calculate the cosine similarity to get the CLIP score
    clip_score = torch.matmul(image_features, text_features.T).item()
    
    return clip_score

timestamp = input("Enter the timestamp (folder name in chatgpt_data) at which your input descriptions were generated: ")

script_dir = os.path.abspath(os.path.dirname(__file__))

#get nr of descriptions
descriptions_path = os.path.join(script_dir, f"chatgpt_data/{timestamp}")
all_entries = os.listdir(descriptions_path)
n_desc = len(all_entries)

clip_scores_global = []
for i in range(0, n_desc):
    image_path = os.path.join(script_dir, f"chatgpt_output/{timestamp}/gc7.5-seed0-alpha0.8/{i*3}_xl_s0.4_n20.png")
    description_path = os.path.join(script_dir, f"chatgpt_data/{timestamp}/chatgpt_descriptions_bboxes{i+1}.json")
    try:
        with open(description_path, 'r') as file:
            full_desc = json.load(file)
        global_desc = full_desc['caption']

        score = get_clip_score(image_path, global_desc)
        clip_scores_global.append(score)
        print(f"CLIP Score for file chatgpt_descriptions_bboxes{i+1}; image {i*3}_xl_s0.4_n20.png: {score}")
    except Exception as e:
        print(f"Error processing file {description_path}: {e}")

mean_score = sum(clip_scores_global)/len(clip_scores_global)
print(f"Mean CLIP score for global image captions: {mean_score:.3f}")